# 网络爬虫是什么

​	简单来说，网络爬虫就是自动从互联网中定向或不定向(搜索引擎)的采集信息的一种程序。

​	网络爬虫有很多种类型，常用的有通用网络爬虫、聚焦网络爬虫等。

​	网络爬虫可以做很多事情，不如通用网络爬虫可以用在搜索引擎中，聚焦爬虫可以从互联网中自动采集信息并代替我们筛选出相关的数据出来。具体来说，网络爬虫经常可以应用在以下方面：

​	1、搜索引擎

​	2、采集金融数据

​	3、采集商品数据

​	4、自动过滤广告

​	5、采集竞争对手的客户数据

​	6、采集行业相关数据，进行数据分析

​	......



# 爬虫的运行原理

​	通用聚焦网络爬虫运行原理

![](./images/爬虫1.png)

# [正则表达式](https://docs.python.org/zh-cn/3/library/re.html#module-contents)

### 原子

原子是正则表达式中最基本的组成单位，每个正则表达式中至少要包含一个原子。常见的原子类型有：

```python
#普通的字符串
import re
strs="demo exec"
pat="demo"
rst=re.search(pat, strs)
print(rst)
#非打印字符
# \n \t
#通用字符作为原子
"""
\w 字母、数字、下划线
\W 除字母、数字、下划线
\d 十进制数字
\D 除十进制数字
\s 空白字符
\S 除空白字符
"""
#原子表
#[element1, element2,...]
```

### 元字符

所谓的元字符，就是这种表达式中具有一些特殊含义的字符，比如重复N次前面的字符等。

```python
"""
. 除换行外任意一个字符
^ 开始位置
$ 结束位置
* 0\1\多次
？0\1次
+ 1\多次
{n} 恰好出现n次
{n,} 至少出现n次
{n,m} 至少n次，至多m次
| 模式选择符
() 模式单元
"""
```

### 模式修正符

所谓模式修正符，即可以在不改变正则表达式的情况下，通过模式修正符改变正则表达式的含义，从而实现一些匹配结果的调整功能。

```python
"""
I 匹配时忽略大小写 *
M 多行匹配 *
S 让.匹配包括换行符 *
L 本地化识别配置
U Unicode
"""
```

### 贪婪模式与懒惰模式

贪婪模式的核心点就是尽肯能多的匹配，而懒惰模式的核心点就是尽可能少的匹配。

```python
string="pythonpython"
pat1="p.*y"
pat2="p.*?y" #懒惰模式，精准
rst1=re.search(pat1,string) # pythonpy
rst2=re.search(pat1,string,re.I) # py
```

### 正则表达式函数

正则表达式函数有re.match()函数、re.search()函数、全局匹配函数、re.sub()函数

```python
#1、match  从头开始匹配，返回一个结果
#2、search 返回第一个匹配的结果
#3、全局匹配函数 返回全部匹配结果
string="fasfsdafasdfasfdasfsdfsfdd"
pat="f.*?f"
rst1=re.match(pat, string) # <re.Match object; span=(0, 4), match='fasf'>
rst2=re.search(pat, string) # <re.Match object; span=(0, 4), match='fasf'>
rst3=re.compile(pat).findall(string) # <re.Match object; span=(0, 4), match='fasf'>
```

### 常见正则实例

```python
string='<a href="http://news.baidu.com" name="tj_trnews" class="mnav">新闻</a>'
pat="[a-zA-Z]+://[^\s]*[.com|.cn]" #todo 原子表中的.实际未起到作用，\.只在外面起作用，在里面未起作用
rst=re.compile(pat).findall(string)
```

简单的爬虫编写

```python
import urllib.request
url="http://www.csdn.net"
data=urllib.request.urlopen(url).read()
```

# [urllib基础](https://docs.python.org/zh-cn/3/library/urllib.request.html)

## [`urllib.parse`](https://docs.python.org/zh-cn/3/library/urllib.parse.html#module-urllib.parse) 

用于解析 URL

**源代码:** [Lib/urllib/parse.py](https://github.com/python/cpython/tree/3.9/Lib/urllib/parse.py)

该模块定义了一个标准接口，用于URL字符串按组件(协议、网络位置、路径等)分解，或将组件组合回URL字符串，并将 "相对URL "转换为给定 "基础URL "的绝对URL。

## [`urllib.request`](https://docs.python.org/zh-cn/3/library/urllib.request.html#module-urllib.request) 

--- 用于打开 URL 的可扩展库

[`urllib.request`](https://docs.python.org/zh-cn/3/library/urllib.request.html#module-urllib.request) 模块定义了适用于在各种复杂情况下打开 URL（主要为 HTTP）的函数和类 --- 例如基本认证、摘要认证、重定向、cookies 及其它。

`urllib.request.urlopen`(*url*, *data=None*, [*timeout*, ]***, *cafile=None*, *capath=None*, *cadefault=False*, *context=None*)

打开统一资源定位地址 *url*，可以是一个字符串或一个 [`Request`](https://docs.python.org/zh-cn/3/library/urllib.request.html#urllib.request.Request) 对象。

`urllib.request.urlretrieve(url, filename=None, reporthook=None, data=None)`

Copy a network object denoted by a URL to a local file. If the URL points to a local file, the object will not be copied unless filename is supplied.

网页,文本文件等存储地址，直接下载到本地

`Request.add_header`(*key*, *val*)

`Request.add_unredirected_header`(*key*, *header*)

添加一个不会被加入重定向请求的头部。

`Request.set_proxy`(*host*, *type*)

`Request.get_header`(*header_name*, *default=None*)

`Request.header_items`()

Return a list of tuples (header_name, header_value) of the Request headers.

## [`urllib.response`](https://docs.python.org/zh-cn/3/library/urllib.request.html#module-urllib.response)

 --- urllib 使用的 Response 类

The [`urllib.response`](https://docs.python.org/zh-cn/3/library/urllib.request.html#module-urllib.response) module defines functions and classes which define a minimal file-like interface, including `read()` and `readline()`. Functions defined by this module are used internally by the [`urllib.request`](https://docs.python.org/zh-cn/3/library/urllib.request.html#module-urllib.request) module. The typical response object is a [`urllib.response.addinfourl`](https://docs.python.org/zh-cn/3/library/urllib.request.html#urllib.response.addinfourl) instance:

## [`urllib.error`](https://docs.python.org/zh-cn/3/library/urllib.error.html#module-urllib.error) 

--- urllib.request 引发的异常类

[`urllib.error`](https://docs.python.org/zh-cn/3/library/urllib.error.html#module-urllib.error) 模块为 [`urllib.request`](https://docs.python.org/zh-cn/3/library/urllib.request.html#module-urllib.request) 所引发的异常定义了异常类。 基础异常类是 [`URLError`](https://docs.python.org/zh-cn/3/library/urllib.error.html#urllib.error.URLError)。

## [`urllib.robotparser`

[](https://docs.python.org/zh-cn/3/library/urllib.robotparser.html#module-urllib.robotparser) --- robots.txt 语法分析程序

此模块提供了一个单独的类 [`RobotFileParser`](https://docs.python.org/zh-cn/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser)，它可以回答关于某个特定用户代理是否能在 Web 站点获取发布 `robots.txt` 文件的 URL 的问题。 有关 `robots.txt` 文件结构的更多细节请参阅 http://www.robotstxt.org/orig.html。

# 超时设置

`urllib.request.urlopen`(*url*, *data=None*, [*timeout*, ]***, *cafile=None*, *capath=None*, *cadefault=False*, *context=None*)

# 自动模拟http

```python
#get请求--实现自动百度信息搜索
import urllib.request,re
keywd="demo"
keywd=urllib.request.quote(keywd) #URL转码
for i in range(10):
    url="http://www.baidu.com/s?wd="+keywd+"&pn="+str((i-1)*10)
    data=urllib.request.urlopen(url).read().decode("utf-8")
    pat1="title:'(.*?),'"
    pat2='"title":""(.*?),"'
    rst1=re.compile(pat1).findall(data)
    rst2=re.compile(pat2).findall(data)
    for j in range(0,len(rst1)):
        print(rst1[j])
    for k in range(0,len(rst2)):
        print(rst1[k])
```



```python
import urllib.request
import urllib.parse
posturl="http://www.iqianyue.com/mypost/"
postdata=urllib.parse.urlencode({
"name":"demo@demo.com",
"pass":"passwd"
})
#进行post，使用urllib.request下面的 Request封装(真实地址，post数据)
req=urllib.request.Request(posturl, postdata)
rst=urllib.request.urlopen(req).read().decode("utf-8")
```

# 爬虫异常处理

爬虫在运行的过程中，很多时候都会遇到这样或那样的异常。如果没有异常处理，爬虫遇到异常就会直接崩溃停止运行，下次再次运行时，又会重头开始，所以，要开发一个具有顽强生命力的爬虫，必须要进行异常处理。

## 常见状态码及含义

## URLError与HTTPError

两者都是异常处理的类，HTTPError是URLError的子类，HTTPError有异常状态码与异常原因，URLError没有异常状态码，所以，在处理的时候，不能使用URLError直接代替HTTPError。如果要代替，必须要判断是否有状态码属性。

URLError出现的原因：

1）连不上服务器

2）远程URL不存在

3）无网络

4）触发HTTPError

```python
import urllib.request
import urllib.error
try:
    urllib.request.urlopen("http://blog.csdn.net")
except urllib.error.URLError as e:
    if hasattr(e,"code"):
        print(e.code)
    if hasattr(e,"reason"):
        print(e.reason)
```

# 浏览器伪装技术原理

服务器对爬虫进行屏蔽，此时我们需要伪装成浏览器才能爬取。

伪装浏览器我们一般通过报头进行。

```python
import urllib.request
uapools=[
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1 QQBrowser/6.9.11079.201", # QQ浏览器极速模式
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3493.3 Safari/537.36",   # 谷歌
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; ) AppleWebKit/534.12 (KHTML, like Gecko) Maxthon/3.0 Safari/534.12",   # 遨游
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0"   # 火狐
    ]
url="http://blog.csdn.net"
#头文件格式 header=("User-Agent"，具体用户代理值)
headers=("User-Agent", uapools[0])
urllib.request.build_opener()
opener=urllib.request.build_opener()
opener.addheaders.append(headers)
data=opener.open(url)
print(data.read().decode("U8"))
```

# 用户代理池构建

在伪装的基础上，随机选取User-Agent来请求

# IP代理池构建

```python
import urllib.request
import random
ippools=["1.1.1.1:996"] #这里也可以是ip代理池api
ip=random.choice(ippools)
proxy=urllib.request.ProxyHandler({"http":ip})
opener=urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
urllib.request.install_opener(opener)
url="http://www/baidu.com"
data=urllib.request.urlopen(url).read().decode("utf-8", "ignore")
print(data)
```

# 淘宝商品图片爬虫实战

观察网址的构造

右键查看源码

 g_page_config = {"pageName":"mainsrp","mods":{"shopcombotip":{"status":"hide"},"phonenav":{"status":"hide"},"debugbar":{"status":"hide"},"shopcombo":{"status":"hide"},"itemlist":{"status":"show","data":{"postFeeText":"运费","trace":"msrp_auction","auctions":[{"p4p":1,"p4pSameHeight":true,"nid":"636915279756","category":"","pid":"","title":"2021春秋新款碎花\u003cspan class\u003dH\u003e连衣裙\u003c/span\u003e女长款v领收腰绑带法式复古温柔风茶歇裙","raw_title":"2021春新款碎花连衣裙女法式复古温柔风茶歇","**pic_url**":"//g-search1.alicdn.com/img/bao/uploaded/i4/imgextra/i2/117742277/O1CN01kAQKcq1SguVAkRhdU_!!0-saturn_solar.jpg".......

![img](.\images\O1CN01kAQKcq1SguVAkRhdU_!!0-saturn_solar.jpg)

图片网址规律

小图

https://img.alicdn.com/imgextra/https://img.alicdn.com/bao/uploaded/i4/604622217/O1CN01nRTMoL1SFQkMyxX1a_!!604622217.jpg_430x430q90.jpg

大图

https://img.alicdn.com/imgextra/https://img.alicdn.com/bao/uploaded/i4/604622217/O1CN01nRTMoL1SFQkMyxX1a_!!604622217.jpg



```python
import urllib.request
import re
import random
keyname="连衣裙"
key=urllib.request.quote(keyname)
uapools=[
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1 QQBrowser/6.9.11079.201", # QQ浏览器极速模式
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3493.3 Safari/537.36",   # 谷歌
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; ) AppleWebKit/534.12 (KHTML, like Gecko) Maxthon/3.0 Safari/534.12",   # 遨游
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0"   # 火狐
    ]

def ua(uapools):
    thisua=random.choice(uapools)
    print(uapools)
    headers=("User-Agent", thisua)
    opener=urllib.request.build_opener()
    opner.addheaders=[headers]
    #安装为全局
    urllib.request.install_opener(opener)

for i in range(1,100):
    #构造请求地址 key为请求关键字，s为每页的展示个数
    url="https://s.taobao.com/search?q="+key+"&s="+str((i-1)*44)
    ua(uapools)
	data=urllib.request.urlopen(url).read().decode("utf-8",  "ingnore")
    pat='"pic+url":"//(.*)"'
    imglist=re.compile(pat).findall(data)
    for j in range(0, len(imglist):
		thisimg=img[j]
		thisimgurl="http://"+thisimg
		localfile="./"+str(i)+str(j)+".jpg"
		urllib.request.urlretrieve(thisimgurl, filename=localfile)


```

# 微信爬虫实战

打开https://weixin.sogou.com/，同样观察网址结构

type=1为公众号

type=2为文章

https://weixin.sogou.com/weixin?type=1&query=arangoDB

https://weixin.sogou.com/weixin?type=2&query=arangoDB

其余的爬器动作同上

# 抓包分析

通过抓包工具代理，抓取HTTPS请求，分析请求的详细细节。

# 自动进行Ajax异步请求数据，爬取腾讯视频评论

通过fiddler，抓取评论的HTTPS请求

https://video.coral.qq.com/varticle/1006590007/comment/v2?callback=_varticle1006590007commentv2&orinum=10&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=132&_=1616843083933

/varticle/1006590007/comment/v2?callback=_varticle1006590007commentv2&orinum=10&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=132&_=1616843083933 HTTP/1.1
Host: video.coral.qq.com

再抓两个分析

https://video.coral.qq.com/varticle/6615020865/comment/v2?callback=_varticle6615020865commentv2&orinum=10&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=132&_=1616843790202
https://video.coral.qq.com/varticle/1006590007/comment/v2?callback=_varticle1006590007commentv2&orinum=10&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=132&_=1616843083933

https://video.coral.qq.com/varticle/6645779685/comment/v2?callback=_varticle6645779685commentv2&orinum=10&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=132&_=1616844195361

最后结果

https://video.coral.qq.com/varticle/{视频id}/comment/v2?orinum={每次提取评论的数目}&pageflag={评论页码}&orirepnum={显示回复评论的数目}

爬虫的代码和其他的没什么区别

# Python网络爬虫之Scrapy框架

![img](.\images\webp)



```python
qimairank

|--qimairank
    |--spiders
        |--__init__.py
    |--__init__.py
    |--items.py
    |--middlewares.py
    |--pipelines.py
    |--settings.py
|--scrapy.cfg
```

`scrapy.cfg`：项目的配置文件，指定`settings`文件，部署deploy的project名称等等。

`qimairank`：项目的python模块。

`spiders`: 爬虫文件

`items.py`: 定义目标

`pipelines.py`: 处理数据

`middlewares.py`：项目的中间件。

`settings.py`：Scrapy 配置文件。更多配置信息查看：[https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/settings.html](https://links.jianshu.com/go?to=https%3A%2F%2Fscrapy-chs.readthedocs.io%2Fzh_CN%2F0.24%2Ftopics%2Fsettings.html)



# Scrapy与Urllib的整合



# PhantomJS无界面浏览器

JS动态触发+id随机生成反爬破解



# 常见的几种分布式爬虫类型

1、多台真机+爬虫(如urllib、Scrapy)+任务共享中心

2、多台虚拟机+爬虫(如urllib、Scrapy)+任务共享中心

3、多台容器级虚拟机+爬虫(如urllib、Scrapy)+任务共享中心

其中任务中心可以采用Redis技术实现

最常用的是

docker+Redis+(urllib or Scrapy)+MySQL

